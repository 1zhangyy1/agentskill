{
  "id": "4a020ab36e21a64b",
  "slug": "cognitive-workbench",
  "name": "Cognitive_workbench",
  "description": "",
  "author": "bdambrosio",
  "authorAvatar": "https://avatars.githubusercontent.com/u/2271133?v=4",
  "repoUrl": "https://github.com/bdambrosio/Cognitive_workbench",
  "repoFullName": "bdambrosio/Cognitive_workbench",
  "stars": 8,
  "forks": 0,
  "category": "other",
  "categories": [
    "other"
  ],
  "tags": [],
  "tier": 3,
  "status": "active",
  "createdAt": "2025-07-22T16:34:16Z",
  "updatedAt": "2025-12-24T00:57:41Z",
  "lastCommitAt": "2025-12-24T00:57:37Z",
  "source": "github-search",
  "collectedAt": "2025-12-24T03:42:54.347Z",
  "authorUrl": "https://github.com/bdambrosio",
  "license": null,
  "readme": "# Cognitive Workbench\n\n**Code as Laboratory for LLM Cognitive Architecture Research**\n\n[![Status: Research Laboratory](https://img.shields.io/badge/status-research_laboratory-purple.svg)]()\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)\n\n---\n\n## üß™ Code as Laboratory\n\n**Cognitive Workbench is not a finished product.** It is a research framework‚Äîa laboratory‚Äîfor exploring the frontiers of LLM-based cognitive architecture.\n\nThe code here is constantly evolving. It prioritizes **experimental capability** over stability. We view the codebase itself as a \"workbench\" where we test hypotheses about agency, memory, planning, and theory of mind.\n\nIf you are looking for a production-ready agent framework, this is likely not it. If you are looking to experiment with novel cognitive architectures, welcome to the lab.\n\n---\n\n## üîÑ Recent Updates (Dec 2025)\n\n- **Abstract pre-planning layer** ‚Äì `IncrementalPlanner` now calls a lightweight `_preplan()` helper that asks the LLM to emit a goal-specific strategy sketch (no tool calls) before the step-by-step planning loop begins. The abstract plan is injected into the planner prompt, giving the LLM a bird's-eye view of the task without constraining downstream execution.\n- **Plan guidance memory** ‚Äì A new `PlanGuidance` module loads historical plan feedback from `src/data/planner_feedback/plan_guidance.jsonl`, embeds the goals, and surfaces high-similarity successes back to the planner. This lets new goals borrow structure from proven strategies while still remaining adaptive.\n- **Tracked feedback corpus** ‚Äì The `plan_guidance.jsonl` file is now committed to the repo so collaborators can reproduce guidance behavior without rebuilding a feedback dataset from scratch. Add your own feedback records (one JSON object per line) to grow the shared memory.\n\n---\n\n## üí° Design Philosophy\n\n**Cognitive Workbench takes a fundamentally different approach to 'reasoning'.**\n\nTraditional (RL) approaches treat agent behavior as a black box: train a policy network end-to-end with reinforcement learning, hope it generalizes, and debug by staring at reward curves. When the agent fails, you adjust hyperparameters and retrain.\n\n**We believe problem-solving strategy should be explicit and inspectable.**\n\nThe core reasoning/planning logic lives in `src/incremental_planner.py`‚Äînot as learned weights, but as structured prompts and execution rules that you can read, understand, and modify directly. When the agent makes a poor decision, you can:\n\n1. **Read the planner trace** (`logs/planner_trace_{character}.txt`) to see exactly what the LLM saw and generated\n2. **Identify the failure mode** (bad tool selection? incorrect reasoning? missing information?)\n3. **Edit the planning specifications** (`INCREMENTAL_PLAN_SPECIFICATIONS`, `PRIMITIVE_DOCS`, `tool_planner_infospace`) to address the issue\n4. **Test immediately** without retraining\n\nThis is **symbolic AI meets LLM capabilities**: the LLM provides language understanding and generation, while the structured planning framework provides guardrails, tool orchestration, and interpretable decision-making.\n\n**Enabled by SGLang's `@function` decorator:** The incremental planning approach is made practical by SGLang's ability to extend context and continue inference at minimal cost. Each planning step appends tool results to the conversation state and resumes generation‚Äîno prompt reconstruction or KV cache rebuilding required. This makes tight plan-execute-observe loops efficient enough for real-time interaction.\n\n**Benefits over RL-based agents:**\n- **Interpretability**: Every decision has a readable rationale\n- **Rapid iteration**: Fix bugs in minutes, not training runs\n- **Composability**: Add new tools without retraining the entire system\n- **Predictability**: Same input ‚Üí same plan (with temperature=0)\n\n**Trade-offs:**\n- Requires manual prompt engineering for new capabilities\n- May not discover novel strategies that RL could find\n- Depends on LLM instruction-following quality\n\nThe benchmarks in `tests/` (MMLU, HotpotQA) serve both as evaluation harnesses and as examples of how to integrate external tasks with the Cognitive Workbench API.\n\n---\n\n## üî¨ Key Research Frontiers\n\nThis workbench currently focuses on three main areas of cognitive agent research:\n\n### 1. Incremental Planning with SGLang\nThe **Incremental Planner** implements **interleaved reasoning** within a single SGLang evolving state:\n- **Plans** incrementally, a few steps at a time\n- **Executes** tools **during planning** to progress on goal\n- **Adapts** the plan based on execution results  \n- **Requests** additional tools dynamically as needs emerge\n- All within SGLang's `@function` context for efficient synchronous execution\n\n```mermaid\nsequenceDiagram\n    participant LLM as SGLang Stream\n    participant Exec as Executor\n    participant Mem as Infospace (Memory)\n    \n    LLM->>LLM: Generate Thought\n    LLM->>LLM: Select Tool (JSON)\n    LLM->>Exec: Call Tool\n    Exec->>Mem: Read/Write (Note/Collection)\n    Mem-->>Exec: Return Result\n    Exec-->>LLM: Inject Result (Text)\n    LLM->>LLM: Reflect & Next Step\n```\n\n**Key Innovation:** Tool execution happens **inside** the planning loop, for any instruct LLM SGLang can run locally. LLM need not be natively capable or reasoning or tool use, in fact better if it is just a plan *instruct* model.  The planner sees actual results (e.g., \"Word count: 306\") and adapts immediately.\n\n**Result Format:** Tools now return status, resource (Note or Collection) ids created, and output (truncated for injection into SGLang state, full for return to caller. Screen output (and in the context):\n```\n[SUCCESS] Word count: 306 | word-count completed | Bound: $count to Note_529\n```\n\nThis tight integration between thought and action enables complex research tasks requiring iterative information gathering and synthesis.\n\n*See: `src/incremental_planner.py`*\n\n[‚ñ∂Ô∏è Incremental planning demo](https://github.com/bdambrosio/Cognitive_workbench/raw/main/docs/incremental_planning.mp4)\n\n### Information Space (\"Infospace\")\nCognitive Workbench supports a working memory that acts like an **Operating System for Thoughts**:\n\n- **Notes (Files):** Persistent data units (text, JSON, code).\n- **Collections (Folders):** Ordered lists of Notes.\n- **Operations (Syscalls):** SQL-like manipulation of memory (split, map, filter, join, sort).\n\nThis allows the agent to treat memory not as a fuzzy context window, but as a structured file system:\n*\"I will save search results to a Collection, filter for 'relevance > 0.8', and summarize the remaining Notes.\"*\n\n*See: `src/infospace_executor.py`, `src/infospace_resource_manager.py`*\n\n### New Interactive Primitives\n\n**`ask` Primitive:**\n- Agent can request information from the user mid-plan\n- Synchronous execution: planning suspends until user responds\n- UI shows pending question indicator in input area\n- Example: `{\"type\": \"ask\", \"target\": \"user\", \"value\": \"Which paper should I focus on?\", \"out\": \"$user_choice\"}`\n\n**`think` Primitive:**\n- Internal reasoning appended to SGLang conversation state\n- Not displayed externally, but logged for debugging\n- Does NOT create Notes (use `generate-note` or `create-note` to persist content)\n- Example: `{\"type\": \"think\", \"value\": \"Should I verify these claims before proceeding?\"}`\n\n*See: `src/infospace_executor.py`*\n\n### 3. Schema-First Tool Definitions\nTools are not just Python functions. They are defined by a **Natural Language Schema (`SKILL.md`)** and a **Python Implementation (`tool.py`)**.\n\n**SKILL.md (The Interface):**\n```markdown\nname: search-web\ndescription: \"Search web using Google CSE. Returns Collection of JSON Notes...\"\nexamples:\n  - '{\"type\":\"search-web\",\"value\":\"transformer architecture papers\",\"out\":\"$results\"}'\n```\n\n**tool.py (The Logic):**\n```python\ndef query_web(query: str, count: int = 10, **kwargs):\n    # Execute search, return structured data\n    return [...]\n```\n\nThe planner uses the schema to understand *how* to use the tool, and the executor maps the JSON action to the Python logic.\n\n*See: `src/tools/`*\n\n### 4. Entity Modeling & Theory of Mind (ToM) (still there from older multi-agent version, not fully functional right now)\nAgents build mental models of their interlocutors:\n- Tracks every interaction indexed by other character (default: \"User\")\n- Maintains distinct **discourse states** for each relationship\n- Models **Theory of Mind (ToM)** ‚Äî what they know, want, and intend\n- Consolidates conversation history to maintain long-term coherence\n\n*See: `src/memory.py`, `src/entity_model.py`, `src/discourse.py`*\n\n---\n\n## üèóÔ∏è Architecture (Current - Nov 2025)\n\nThe system is 100% **Infospace-only** (physical world support removed). It uses **Zenoh** for inter-process communication and **SGLang** for LLM inference.\n\n### Core Components\n\n| Component | Responsibility |\n|-----------|----------------|\n| **Executive Node** | OODA loop, planning coordination, goal management, memory integration |\n| **Incremental Planner** | SGLang-based synchronous planning with in-loop tool execution |\n| **Infospace Executor** | Executes primitives (create, load, search, ask, think) and tools |\n| **Resource Manager** | Manages Notes/Collections, vector indexing, semantic search |\n| **Memory Module** | Entity models, discourse tracking, conversation history |\n| **FastAPI UI** | Web interface for monitoring and interaction |\n\n### Recent Removals (Nov 2025)\n- ~~Memory Node~~ ‚Üí Integrated into Executive Node as `memory.py` module\n- ~~Map Node~~ ‚Üí Functionality moved to `InfospaceResourceManager` (direct method calls)\n- ~~Perception Node~~ ‚Üí Refactored to `action_post_processing.py` utility\n- ~~Semantic Validator~~ ‚Üí Validation now handled by Incremental Planner\n- ~~Physical World~~ ‚Üí System is 100% infospace-only\n- ~~CharacterRAGStore~~ ‚Üí Removed (unused)\n- ~~UnifiedPlanner~~ ‚Üí Removed (redundant abstraction layer)\n- ~~Activity System~~ ‚Üí Removed (activity_manager, activity UI tabs, activity.py)\n- ~~Turn-based Execution~~ ‚Üí Continuous OODA loop with direct planning\n- ~~Simulation Time~~ ‚Üí Now uses real `datetime.datetime`\n\n### Technology Stack\n- **Communication**: [Eclipse Zenoh](https://zenoh.io/) (Python) - for inter-process pub/sub\n- **LLM Backend**: [SGLang](https://github.com/sgl-project/sglang) (required for planning)\n- **Planning**: SGLang `@function` with constrained generation, synchronous execution\n- **Vector Store**: `txtai` with `sentence-transformers` embeddings\n- **Tools**: 32+ dynamically loaded skills in `src/tools/`\n- **UI**: FastAPI + WebSockets for real-time monitoring\n\n---\n\n## üéØ Example: Complex Research Task\n\n**Goal:** \"I need to understand the debate around emergent capabilities in large models. Find arguments on both sides, assess their evidence quality, and tell me what questions remain unresolved.\"\n\n**System Response:** 16-step incremental plan with adaptive reasoning:\n\n```\nüì• Jill queued goal ‚Üí Incremental planner activated\n\nFIRST_TASK: Conduct initial web and academic literature search\n\nStep 0: search-web ‚Üí [SUCCESS] Bound $web_results to Collection_95 (6 items)\n  THOUGHTS: Should summarize to understand key arguments, then proceed to academic papers\n  DONE: NO\n  NEXT_TASK: Summarize web search results\n  REQUEST_TOOLS: [\"summarize\", \"create-collection\"]\n\nStep 1: summarize ‚Üí [SUCCESS] Bound $web_summary to Note_334\n  THOUGHTS: Web results summarized. Now need academic papers for detailed arguments\n  DONE: NO\n  NEXT_TASK: Conduct academic literature search\n  REQUEST_TOOLS: [\"semantic-scholar\"]\n\nStep 2: semantic-scholar ‚Üí [SUCCESS] Bound $papers to Collection_96 (9 papers)\n  THOUGHTS: 9 academic papers found. Should summarize to understand core arguments\n  DONE: NO\n  NEXT_TASK: Summarize academic papers for key arguments and evidence\n  REQUEST_TOOLS: [\"summarize\"]\n\nStep 3: summarize ‚Üí [SUCCESS] Bound $paper_summary to Note_344\n  THOUGHTS: Papers summarized. Need to examine arguments closely, look for contrasts\n  DONE: NO\n  NEXT_TASK: Identify key papers representing both sides, focus on evidence quality\n  REQUEST_TOOLS: [\"create-collection\", \"relate\", \"extract-entities\"]\n\nStep 4: extract-entities ‚Üí [SUCCESS] Bound $entities to Note_345\n  THOUGHTS: Entities extracted. Create focused collection for core debate papers\n  DONE: NO\n  NEXT_TASK: Create focused collection, examine contrasting arguments\n  REQUEST_TOOLS: [\"create-collection\", \"filter-collection\"]\n\nStep 5: create-collection ‚Üí [SUCCESS] Bound $focused_papers to Collection_97\n  THOUGHTS: Collection wrapped in another collection. Need to filter for key papers\n  DONE: NO\n  NEXT_TASK: Refine to papers explicitly addressing emergence debate\n  REQUEST_TOOLS: [\"map\", \"filter-collection\", \"flatten\"]\n\nStep 6: flatten ‚Üí [SUCCESS] Bound $flattened_papers to Note_346\n  THOUGHTS: Need to see actual titles/years. Use map to extract metadata\n  DONE: NO\n  NEXT_TASK: Extract metadata (titles, years) for targeted filtering\n  REQUEST_TOOLS: [\"map\"]\n\nStep 7: map ‚Üí [SUCCESS] Bound $paper_metadata to Collection_98 (9 items)\n  THOUGHTS: Metadata extracted. Filter for papers with strong emergence claims\n  DONE: NO\n  NEXT_TASK: Filter for key papers addressing core debate\n  REQUEST_TOOLS: [\"filter-collection\"]\n\nStep 8: filter-collection ‚Üí [SUCCESS] Bound $debate_papers to Collection_99 (9 items)\n  THOUGHTS: All 9 papers discuss emergence. Need different approach for contrasts\n  DONE: NO\n  NEXT_TASK: Focus on papers with arguments for vs. against emergence\n  REQUEST_TOOLS: [\"display\"]\n\nStep 9: display ‚Üí [SUCCESS] display completed\n  THOUGHTS: Display didn't show full papers. Analyze structure, extract arguments\n  DONE: NO\n  NEXT_TASK: Analyze key paper to understand debate structure\n  REQUEST_TOOLS: [\"load\", \"summarize\", \"relate\", \"create-note\"]\n\nStep 10: load ‚Üí [ERROR] load failed: No response for Collection_96\n  THOUGHTS: Can't load collection directly. Return to web results for details\n  DONE: NO\n  NEXT_TASK: Use web results for papers addressing for/against arguments\n  REQUEST_TOOLS: [\"display\", \"create-collection\"]\n\nStep 11: display ‚Üí [SUCCESS] display completed\n  THOUGHTS: Need systematic approach. Synthesize current understanding now\n  DONE: NO\n  NEXT_TASK: Synthesize understanding focusing on arguments and evidence quality\n  REQUEST_TOOLS: [\"create-note\", \"summarize\"]\n\nStep 12: create-note ‚Üí [SUCCESS] Bound $synthesis to Note_356\n  THOUGHTS: Synthesized key arguments. Should examine strong pro/con paper\n  DONE: NO\n  NEXT_TASK: Select paper representing emergence or non-emergence perspective\n  REQUEST_TOOLS: [\"load\", \"search-web\"]\n\nStep 13: search-web ‚Üí [SUCCESS] Bound $emergence_papers to Collection_100 (6 items)\n  THOUGHTS: Found pro-emergence papers. Now search anti-emergence for comparison\n  DONE: NO\n  NEXT_TASK: Search papers arguing against emergence\n  REQUEST_TOOLS: [\"search-web\"]\n\nStep 14: search-web ‚Üí [SUCCESS] Bound $anti_emergence_papers to Collection_101 (4 items)\n  THOUGHTS: Have both sides. Summarize understanding of debate and evidence quality\n  DONE: NO\n  NEXT_TASK: Create comprehensive summary with arguments, evidence, open questions\n  REQUEST_TOOLS: [\"summarize\", \"create-note\"]\n\nStep 15: create-note ‚Üí [SUCCESS] Bound $final_analysis to Note_367\n  THOUGHTS: Completed analysis of debate from both sides with evidence assessment\n  DONE: YES ‚úì\n  NEXT_TASK: Present final analysis to user\n  REQUEST_TOOLS: [\"display\"]\n```\n\n**Final Output (Note_367):**\n```markdown\n## Analysis of the Emergent Capabilities Debate in Large Language Models\n\n### Arguments for Emergence:\n1. Capabilities appear suddenly at scale rather than building linearly\n2. Some abilities only manifest in very large models (like reasoning)\n3. These capabilities aren't simply extrapolations from smaller models\n4. Complex emergent behaviors are often surprising to developers\n5. Qualitative differences in performance across model sizes\n\n### Arguments Against Emergence:\n1. All capabilities can be attributed to scaling laws and training\n2. No true 'new' abilities - just better extrapolation of existing patterns\n3. Emergent behavior might be explained by improved training methods\n4. Claims of emergence may reflect imagination rather than genuine new capabilities\n5. The phenomenon can be fully understood through current theoretical frameworks\n\n### Evidence Quality:\n- **Strong evidence** for scaling relationships and performance improvements\n- **Mixed evidence** for genuine emergence vs. extrapolation\n- **Methodological differences** between empirical and theoretical approaches\n- **Peer review varies** significantly in quality and depth of analysis\n- **Replication concerns** in some empirical studies\n\n### Remaining Questions:\n1. How do we distinguish genuine emergence from improved scaling?\n2. What constitutes \"emergence\" in ML context?\n3. How should we evaluate evidence quality for emergence claims?\n4. Are our theoretical frameworks adequate for studying underlying mechanisms?\n5. What are the implications for general intelligence research?\n\n### Conclusion:\nThe debate remains unresolved due to methodological disagreements and unclear \ndefinitions. The core issue is whether observed capabilities represent a \nqualitative leap beyond scaling or just the amplification of existing patterns.\n```\n\n**Key Capabilities Demonstrated:**\n- Multi-source information gathering (web + academic)\n- Iterative refinement based on results\n- Tool composition (summarize ‚Üí extract-entities ‚Üí filter)\n- Evidence quality assessment\n- Synthesis of conflicting viewpoints\n\n---\n\n## üöÄ Setting Up the Lab\n\n### Prerequisites\n- Python 3.10+\n- GPU recommended for SGLang (CPU fallback available)\n\n### Installation\n```bash\n# Clone the laboratory\ngit clone https://github.com/bdambrosio/Cognitive_workbench.git\ncd Cognitive_workbench\n\n# Initialize environment\npython3 -m venv zenoh_venv\nsource zenoh_venv/bin/activate\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n### Configuration\nCreate src/logs/ (needed dir, autocreate on todo)\nEdit `scenarios/jill.yaml`:\n\n**Minimal Configuration:**\n```yaml\nllm_config:\n  sgl_model_path: \"/path/to/model\"  # Required - path to your SGLang model\n\ncharacters:\n  Jill:\n    character: |\n      Character description...\n    drives:\n      - Goal 1\n      - Goal 2\n```\n\n**SGLang Model Path:** This is the only required LLM configuration. Point it to a local model compatible with SGLang (e.g., Qwen, Llama, etc.).\n\n**Removed Configurations:**\n- `server_name`, `model_name` - No longer used (SGLang only)\n- Activity ontology files (`*-activity-ontology.json`)\n- Activities list files (`*-activities.json`)\n- Simulation time configuration\n\n### Running an Experiment\n```bash\ncd src\npython3 launcher.py jill.yaml --ui --resource-browser --map-file infolab.py\nStartup can take a while, it loads llm, be patient...\n\nAccess the UI at `http://localhost:3000`.\nSee also benchmarks below (tests/hotpotqa_eval.py, tests/mmlu_eval.py, tests/gaia_eval.py, tests/chat_cli.py), for examples of use of the zenoh api provided by launcher.py\n\n---\n\n## üìÇ Repository Structure\n\n<pre>\nsrc/\n‚îú‚îÄ‚îÄ executive_node.py           # Main OODA loop, SGLang runtime, planning coordination\n‚îú‚îÄ‚îÄ incremental_planner.py      # SGLang-based synchronous planning with tool execution\n‚îú‚îÄ‚îÄ infospace_executor.py       # Primitive & tool execution engine\n‚îú‚îÄ‚îÄ infospace_resource_manager.py  # Note/Collection persistence, vector search\n‚îú‚îÄ‚îÄ memory.py                   # Entity model & discourse integration\n‚îú‚îÄ‚îÄ entity_model.py            # Theory of Mind tracking per entity\n‚îú‚îÄ‚îÄ discourse.py               # Conversation state analysis\n‚îú‚îÄ‚îÄ fastapi_action_display.py  # Web UI server (FastAPI + WebSockets)\n‚îú‚îÄ‚îÄ launcher.py                # Process orchestration\n‚îú‚îÄ‚îÄ templates.py               # LLM prompt templates\n‚îú‚îÄ‚îÄ utils/\n‚îÇ   ‚îú‚îÄ‚îÄ action_post_processing.py  # Result validation utilities\n‚îÇ   ‚îî‚îÄ‚îÄ llm_api.py             # LLM client wrapper (legacy)\n‚îî‚îÄ‚îÄ tools/                     # 32+ dynamically loaded skills\n    ‚îú‚îÄ‚îÄ search-web/             # Web search & extraction\n    ‚îú‚îÄ‚îÄ semantic-scholar/      # Academic paper search\n    ‚îú‚îÄ‚îÄ summarize/             # Text summarization\n    ‚îú‚îÄ‚îÄ filter-collection/     # Semantic filtering\n    ‚îú‚îÄ‚îÄ relate/                # Relationship analysis\n    ‚îú‚îÄ‚îÄ word-count/            # Text metrics\n    ‚îî‚îÄ‚îÄ ...\n\ntests/\n‚îú‚îÄ‚îÄ mmlu_eval.py               # MMLU benchmark harness (see Benchmarks section)\n‚îî‚îÄ‚îÄ hotpotqa_eval.py           # HotpotQA benchmark harness\n\nscenarios/                     # Agent configuration YAML files\ndata/                         # Persistent storage (Notes, Collections, memory)\nlogs/                         # Execution logs and planner traces\nDocs/                         # Design documents (may lag code)\n</pre>\n\n---\n\n## üõ†Ô∏è Available Tools & Primitives\n\n**Information Gathering (tools):**\n- `search-web` - Web search with LLM-based extraction\n- `semantic-scholar` - Academic paper search\n- `fetch-text` - Fetch full text from URL or paper ID\n- `search-notes` - Semantic search over existing Notes\n- `search-collections` - Search Collections\n- `search-within-collection` - Search within a specific Collection\n\n**Transformation (tools & primitives):**\n- `summarize` - Hierarchical text summarization\n- `filter-collection` - Semantic filtering with LLM predicates\n- `map` - Apply operation to each item in Collection (supports additional args like `focus`, `max_length`)\n- `expand` - Split Note into Collection (lines/JSON/etc.)\n- `flatten` - Merge Collection items into single Note\n- `relate` - Compare/relate multiple items\n- `coerce` - Convert between formats\n\n**Structured Data Operations (primitives):**\n- `project` - Extract metadata fields from Notes or fields from structured Notes (not text content‚Äîuse `refine` for that)\n- `pluck` - Extract single field values from Notes\n- `filter-structured` - Filter by field conditions (SQL WHERE)\n- `sort` - Order Collection by field (SQL ORDER BY)\n- `join` - Combine Collections on matching field (SQL JOIN)\n- `head` - Take first N items from Collection (SQL LIMIT)\n\n**Set Operations (primitives):**\n- `size` - Count items in Collection\n- `union` - Combine two Collections\n- `intersection` - Items in both Collections\n- `difference` - Items in first but not second Collection\n- `add` - Add item to Collection\n- `remove` - Remove item from Collection\n\n**Analysis (tools):**\n- `extract-entities` - Named entity extraction\n- `extract-struct` - Structured data extraction\n- `assess` - Quality/relevance assessment\n- `as-json` - Parse/validate JSON content\n- `as-markdown` - Convert to markdown format\n- `word-count` - Text metrics\n- `is-empty` - Check if content is empty\n- `is-positive` - Sentiment check\n- `is-question` - Check if text is a question\n- `matches` - Pattern matching\n- `text-find` - Find text within content\n\n**Generation (tools & primitives):**\n- `generate-note` - LLM-generated content (new text, code, analysis)\n- `refine` - Extract/transform information from unstructured text (does not add new content)\n- `create-note` - Create Note with literal content\n- `create-collection` - Create Collection from items\n\n**Core Primitives (built-in):**\n- `load` - Load Note or Collection by ID or name\n- `persist` - Save Note or Collection to disk\n- `display` - Show content in UI popup\n- `say` - Output text to user (appears in action log)\n- `think` - Internal reasoning (appended to planner context)\n- `ask` - Interactive user input (synchronous wait for response)\n- `index` - Build vector index for Collection search\n- `apply` - Execute a tool on a target\n\n---\n\n## üìä Benchmarks\n\nCognitive Workbench includes evaluation harnesses for standard QA benchmarks. These serve dual purposes:\n1. **Benchmark evaluation**: Measure agent performance on established datasets\n2. **Integration examples**: Demonstrate how to connect external systems to the Cognitive Workbench API\n\n### MMLU Evaluation (`tests/mmlu_eval.py`)\n\nEvaluates on the [MMLU](https://huggingface.co/datasets/cais/mmlu) multiple-choice benchmark.\n\n**Modes:**\n- **Direct mode** (default): Uses Zenoh LLM API for raw generation‚Äîtests the underlying model\n- **Executive mode** (`--use-executive`): Sends goals through the full planner/executor pipeline‚Äîtests the agent\n\n```bash\n# Direct LLM evaluation (requires running Jill session)\ncd src\npython ../tests/mmlu_eval.py --subjects \"high_school_physics\" --k-shot 5 --max-test-per-subject 20\n\n# Full agent evaluation\npython ../tests/mmlu_eval.py --subjects \"high_school_physics\" --use-executive --max-test-per-subject 10\n```\n\n**Key features:**\n- Canonical 5-shot (or 0-shot) prompting per subject\n- Clears transient Notes between questions\n- Extracts \"ANSWER: X\" format from model output\n\n### HotpotQA Evaluation (`tests/hotpotqa_eval.py`)\n\nEvaluates on [HotpotQA](https://huggingface.co/datasets/hotpot_qa), a multi-hop reasoning benchmark with supporting context paragraphs.\n\n**Context modes:**\n- **inline**: Context paragraphs included directly in the goal text\n- **preload**: Context stored as Notes via Zenoh API before the question‚Äîtests the agent's memory retrieval\n\n```bash\n# Inline context (simpler, faster)\npython ../tests/hotpotqa_eval.py --character Jill --max-questions 20 --context-mode inline\n\n# Preload context (tests search-notes capability)\npython ../tests/hotpotqa_eval.py --character Jill --max-questions 10 --context-mode preload\n```\n\n**Evaluation metrics:**\n- **Exact Match (EM)**: Normalized prediction equals gold answer\n- **Contains Match (CM)**: Gold answer appears in prediction\n- **LLM Judge (LJ)**: LLM evaluates semantic correctness\n- **F1 Score**: Token-level overlap\n\n### Zenoh APIs for External Integration\n\nBoth benchmarks demonstrate the Zenoh APIs added for external tool integration:\n\n| API Endpoint | Purpose |\n|--------------|---------|\n| `cognitive/{character}/llm/generate` | Direct LLM generation (bypasses planner) |\n| `cognitive/{character}/resource/create_note` | Create Notes programmatically |\n| `cognitive/{character}/resource/clear_transient` | Clear non-persistent Notes/Collections |\n\nThese APIs enable building custom evaluation harnesses or integrating Cognitive Workbench with external systems.\n\n---\n\n## ‚ö†Ô∏è Current Limitations\n\n1. **SGLang Required** - No longer optional; needed for planning\n2. **No physical world simulation** (removed Nov 2025)\n3. **Single-agent focus** (multi-agent capabilities reduced)\n4. **Memory persistence** is simple JSON (no graph DB)\n5. **UI is minimal** (research tool, not production-ready)\n6. **Documentation lags code** by design (code = truth)\n7. **LLM arithmetic unreliable** - Physics/math problems may get wrong numerical answers; consider adding a calculator tool for precise computation\n\n---\n\n## üö® Breaking Changes (Nov 2025)\n\nIf migrating from an earlier version:\n\n**Removed:**\n- Activity system (activity_manager, activity ontology, activity.json files)\n- UnifiedPlanner (use IncrementalPlanner directly)\n- Simulation time (use real datetime)\n- Map node queryables (use resource_manager direct calls)\n- Physical world support\n- `server_name` and `model_name` in llm_config (use `sgl_model_path` only)\n- `parameter_source` mechanism in tool definitions (all parameters now flat)\n- Nested `args` structure in action JSON\n\n**Changed:**\n- Tool result format (now includes actual values)\n- `think` primitive now creates Notes (not just logs)\n- `ask` primitive works synchronously (polls for response)\n- Tools receive `resource_manager` as kwarg\n- Startup is faster (planners initialized once)\n- **Action format simplified**: All tool parameters are now top-level fields\n  - Old: `{\"type\": \"search-web\", \"args\": {\"query\": \"...\"}}`\n  - New: `{\"type\": \"search-web\", \"query\": \"...\"}`\n- `search-notes` now returns full indexed text content (not 200-char preview)\n\n**Migration Guide:**\n1. Update `llm_config` in YAML - only `sgl_model_path` needed\n2. Remove activity-related files (`*-activity-ontology.json`, `*-activities.json`)\n3. SGLang is now required (install if not present)\n4. Tool implementations may need `resource_manager` parameter\n5. UI will only show \"Plan\" tab (Activity tabs removed)\n\n---\n\n## üî¨ Research Notes\n\n### Recent Architectural Changes (Nov 2025)\n\n**Major Simplifications:**\n\n1. **Removed UnifiedPlanner abstraction** (redundant layer)\n   - `IncrementalPlanner` and `InfospacePlanner` now initialized directly in `executive_node`\n   - Eliminated startup delay from repeated planner instantiation\n   - Cleaner architecture with fewer indirection layers\n\n2. **Eliminated Activity System** (activity_manager, activity.py, activity UI tabs)\n   - System is now purely goal/plan-based\n   - Removed activity ontology, activity selection, activity tracking\n   - Simplified UI: only \"Plan\" tab remains in character panel\n   - Removed ~500 lines of dead code\n\n3. **Resource Access Refactoring**\n   - Replaced Zenoh queries with direct method calls to `resource_manager`\n   - Tools now receive `resource_manager` as kwarg parameter\n   - Fixed broken `load`, `generate-note`, `relate`, `fetch-text` tools\n   - Eliminated map_node queryables (no longer needed)\n\n4. **Removed Simulation Time**\n   - Now uses real `datetime.datetime` throughout\n   - Simplified time handling (no time advancement coordination)\n   - Removed time subscribers and publishers\n\n5. **Improved Tool Result Reporting**\n   - Results now show actual values, not just status\n   - Format: `[SUCCESS] <actual_result> | <action> | Bound: <var> to <resource>`\n   - 128-char truncation for long results\n   - Planner sees both execution status and actual data\n\n6. **Simplified Action Format**\n   - Removed nested `args` structure from action JSON\n   - All tool parameters are now top-level fields\n   - Removed `parameter_source` mechanism from tool loader\n   - Reduces LLM confusion and parsing complexity\n\n7. **Added External Zenoh APIs**\n   - `cognitive/{character}/llm/generate`: Direct LLM access for external tools\n   - `cognitive/{character}/resource/create_note`: Programmatic Note creation\n   - `cognitive/{character}/resource/clear_transient`: Clear non-persistent Notes/Collections\n   - Enables benchmark harnesses and external system integration\n\n8. **Fixed Note/Collection ID Numbering**\n   - Counters now correctly reset on startup based on highest existing IDs\n   - Prevents ID collisions after loading saved state\n\n**Benefits:**\n- 40% reduction in inter-process communication\n- Simpler debugging (fewer nodes, direct calls)\n- Faster startup (single planner initialization)\n- Better planner feedback (sees actual tool results)\n- Cleaner codebase (~700 lines of dead code removed)\n- External tools can access LLM and memory system\n\n**Trade-offs:**\n- Less modularity (acceptable for research)\n- Executive node is larger (~2600 lines)\n- SGLang is now required (not optional)\n\n---\n\n## üìä Performance Characteristics\n\n**Typical Complex Task (16 steps):**\n- Total time: ~45-60 seconds (improved with direct method calls)\n- Web searches: 2-4 seconds each\n- Semantic Scholar: 1 second\n- Summarization: 1-8 seconds (depends on input size)\n- Vector indexing: 300-500 embeddings/sec (txtai + sentence-transformers)\n- SGLang inference: Context-dependent (batch size 1, GPU)\n- Startup: ~15-20 seconds (improved with single planner init)\n\n**Resource Usage:**\n- Memory: ~2-4GB (without SGLang)\n- SGLang: +8-96GB GPU memory (model-dependent)\n  - Recommended: Qwen2.5-32B or Qwen3-Coder-30B (16-bit, ~60GB VRAM)\n  - Configure in `jill.yaml`: `sgl_model_path: \"/path/to/model\"`\n- Disk: ~50MB per agent (Note/Collection persistence)\n- Logs: Planner traces in `logs/planner_trace_{character}.txt` (full conversation state)\n\n**Recent Improvements:**\n- 40% reduction in Zenoh query overhead (direct method calls)\n- Faster startup (single planner initialization vs repeated instantiation)\n- Better planner feedback (actual tool results visible)\n- Reduced shutdown time (5s vs 10s)\n\n---\n\n## üìù Citation\n\nIf you use this workbench for your research, please cite:\n\n```bibtex\n@software{cognitive_workbench,\n  author = {Bruce D'Ambrosio},\n  title = {Cognitive Workbench: A Framework for LLM-Powered Cognitive Agents},\n  year = {2024-2025},\n  url = {https://github.com/bdambrosio/Cognitive_workbench}\n}\n```\n\n---\n\n## ü§ù Contributing\n\nThis is a research laboratory. Contributions are welcome, but expect frequent breaking changes. The best way to contribute:\n\n1. **Experiment** with the code\n2. **Document** your findings (even if informal)\n3. **Share** interesting results or failure modes\n4. **Propose** architectural changes with rationale\n\nCode quality standards are intentionally relaxed to prioritize research velocity.\n\n---\n\n## üìÑ License\n\nMIT License - See LICENSE file for details.\n\n**Disclaimer:** This is experimental research software. It may break, change direction, or be completely rewritten. Use at your own risk.\n",
  "installCommand": "git clone https://github.com/bdambrosio/Cognitive_workbench ~/.claude/skills/cognitive-workbench",
  "defaultBranch": "main",
  "hasMarketplaceJson": false,
  "skillPath": "README.md"
}