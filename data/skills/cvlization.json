{
  "id": "fa605c828457fdb7",
  "slug": "cvlization",
  "name": "CVlization",
  "description": "Practical workflows for using and training vision and language models.",
  "author": "kungfuai",
  "authorAvatar": "https://avatars.githubusercontent.com/u/29697718?v=4",
  "repoUrl": "https://github.com/kungfuai/CVlization",
  "repoFullName": "kungfuai/CVlization",
  "stars": 10,
  "forks": 2,
  "category": "automation",
  "categories": [
    "automation"
  ],
  "tags": [
    "ai",
    "decentralized",
    "docker",
    "finetune",
    "generative-ai",
    "inference",
    "llm",
    "nlp",
    "rl",
    "vision"
  ],
  "tier": 3,
  "status": "active",
  "createdAt": "2021-10-07T03:21:03Z",
  "updatedAt": "2025-12-15T22:01:41Z",
  "lastCommitAt": "2025-12-16T02:21:26Z",
  "source": "github-search",
  "collectedAt": "2025-12-16T03:42:36.713Z",
  "authorUrl": "https://github.com/kungfuai",
  "license": "MIT",
  "readme": "# CVlization: Curated AI Training and Inference Recipes That Just Work\n\nA curated collection of 90+ state-of-the-art open source AI capabilities, packaged in self-contained Docker environments. Find a recipe, test it, copy what you need.\n\n**Browse:** Vision (classification, detection, segmentation, OCR) ‚Ä¢ Language (LLMs, fine-tuning) ‚Ä¢ Multimodal (VLMs, document AI) ‚Ä¢ Generative (image, video) ‚Ä¢ Analytical (time series, tabular ML) ‚Ä¢ Agentic (RAG, tool use, optimization)\n\n*CVlization stands on the shoulders of giants - we package and test amazing open source projects so you can use them with confidence.*\n\n## Quick Start\n\n```bash\ngit clone --depth 1 https://github.com/kungfuai/CVlization\ncd CVlization\n\n# Install CLI (optional - or just use bash scripts)\npip install .\n\n# Browse examples\ncvl list --tag pytorch    # search by tag\ncvl list -k gpt           # search by keyword\n# or browse examples/ on GitHub\n\n# Run any example\ncvl run image-classification-torch build\ncvl run image-classification-torch train\n\n# Copy into your project (bundled with cvlization)\ncvl export perception/image_classification/torch -o your-project/\n```\n\nThat's it! Each example is self-contained with its own Dockerfile and dependencies. (We battled CUDA versions and dependency conflicts so you don't have to.)\n\n## Table of Contents\n\n- [Examples](#examples)\n- [Browse Examples](#browse-examples)\n- [Running an Example](#running-an-example)\n- [Centralized Caching](#centralized-caching)\n- [Requirements](#requirements)\n- [For Contributors](#for-contributors)\n- [CVlization Library](#cvlization-library)\n- [Documentation](#documentation)\n- [License](#licenses)\n\n## Examples\n\nOur `examples/` directory is organized by capability (what the model does) rather than modality (what data it processes). Each example is self-contained with its own Dockerfile and dependencies.\n\n### Directory Structure\n\n```\nexamples/\n  analytical/          # Prediction & forecasting (time series, tabular ML)\n  perception/          # Understand signals (vision, speech, multimodal)\n  generative/          # Create content (text, images, video, audio)\n  agentic/             # AI agents (RAG, tool use, optimization, workflows)\n```\n\n### Catalog of Examples\n\n**Legend:** ‚úÖ = Tested and maintained | üß™ = Experimental\n\n#### üîç Perception (Understanding Signals)\n\n| Capability | Example Directory | Implementations | Status |\n|------------|-------------------|-----------------|--------|\n| ![Image Classification](./doc/images/plant_classification.png) Image Classification | [`examples/perception/image_classification`](./examples/perception/image_classification) | torch | ‚úÖ |\n| ![Object Detection](./doc/images/object_detection.jpg) Object Detection | [`examples/perception/object_detection`](./examples/perception/object_detection) | mmdet, torchvision | ‚úÖ |\n| ![Segmentation](./doc/images/semantic_segmentation.png) Segmentation | [`examples/perception/segmentation`](./examples/perception/segmentation) | instance (mmdet, torchvision), semantic (mmseg, torchvision), panoptic (mmdet, torchvision), sam (experimental), sam3_finetuning | ‚úÖ |\n| ![Pose Estimation](./doc/images/pose_estimation.jpeg) Pose Estimation | [`examples/perception/pose_estimation`](./examples/perception/pose_estimation) | dwpose, mmpose | ‚úÖ |\n| ![Object Tracking](./doc/images/player_tracking.gif) Tracking | [`examples/perception/tracking`](./examples/perception/tracking) | global_tracking_transformer, soccer_visual_tracking | ‚úÖ |\n| ![Line Detection](./doc/images/line_detection.png) Line Detection | [`examples/perception/line_detection`](./examples/perception/line_detection) | torch | ‚úÖ |\n| ![Document AI](./doc/images/layoutlm.png) Document AI | [`examples/perception/doc_ai`](./examples/perception/doc_ai) | OCR (chandra_ocr, deepseek_ocr, docling, doctr, dots_ocr, nanonets_ocr, olmocr_2, paddleocr_vl, surya), VLMs (donut_doc_classification, donut_doc_parse, granite_docling, granite_docling_finetune), Leaderboard (leaderboard) | ‚úÖ |\n| ![Vision-Language](./doc/images/layoutlm.png) Vision-Language Models | [`examples/perception/vision_language`](./examples/perception/vision_language) | florence_2, internvl3, minicpm_v_2_6, moondream2 (+ finetune), moondream3, phi_3_5_vision_instruct, phi_4_multimodal_instruct, qwen3_vl, gemma3_vision_grpo, gemma3_vision_sft | ‚úÖ |\n| ![3D: rendering and reconstruction](./doc/images/nerf.gif) 3D Reconstruction | [`examples/perception/3d_reconstruction`](./examples/perception/3d_reconstruction) | nerf_tf (experimental) | üß™ |\n\n#### ‚ú® Generative (Creating Content)\n\n| Capability | Example Directory | Implementations | Status |\n|------------|-------------------|-----------------|--------|\n| ![LLMs](./doc/images/llm.png) LLMs (text generation) | [`examples/generative/llm`](./examples/generative/llm) | Small-scale pretraining (nanogpt, modded_nanogpt, nanochat), Fine-tuning (peft_mistral7b_sft, trl_sft, unsloth: gpt_oss_grpo, gpt_oss_sft, llama_3b_sft, qwen_7b_sft, gemma3_4b_sft), Inference (mixtral8x7b) | ‚úÖ |\n| ![Image Generation](./doc/images/controlnet.png) Image Generation | [`examples/generative/image_generation`](./examples/generative/image_generation) | cfm, ddpm, diffuser_unconditional, dit, dreambooth, edm2, flux, next_scene_qwen, pixart (experimental), stable_diffusion, uva_energy (experimental), vqgan | ‚úÖ |\n| ![Video Generation](./doc/images/sora.gif) Video Generation | [`examples/generative/video_generation`](./examples/generative/video_generation) | animate_x, cogvideox, framepack, mimic_motion, minisora, phantom (experimental), skyreals, svd_cog, vace, vace_comfy (experimental), video_in_between, wan2gp, wan_comfy, krea_realtime_scope (experimental) | ‚úÖ |\n\n#### üìä Analytical (Prediction & Forecasting)\n\n| Capability | Example Directory | Implementations | Status |\n|------------|-------------------|-----------------|--------|\n| **Time Series Forecasting** | [`examples/analytical/time_series`](./examples/analytical/time_series) | Foundation models (chronos_zero_shot, moirai_zero_shot, uni2ts_finetune (experimental)), Supervised (patchtst_supervised), Statistical baselines (statsforecast_baselines), Hierarchical (hierarchical_reconciliation), Anomaly detection (anomaly_transformer, merlion_anomaly_dashboard) | ‚úÖ |\n| **Tabular ML - AutoML** | [`examples/analytical/tabular/automl`](./examples/analytical/tabular/automl) | autogluon_structured, pycaret_structured | ‚úÖ |\n| **Tabular ML - Causal Inference** | [`examples/analytical/tabular/causal`](./examples/analytical/tabular/causal) | causalml_campaign_optimization, dowhy_berkeley_bias, dowhy_policy_uplift, econml_heterogeneous_effects | ‚úÖ |\n| **Tabular ML - Uncertainty Quantification** | [`examples/analytical/tabular/uncertainty`](./examples/analytical/tabular/uncertainty) | Conformal (conformal_lightgbm, mapie_conformal), Quantile (catboost_quantile, quantile_lightgbm), Bayesian (pymc_bayesian_regression) | ‚úÖ |\n| **Tabular ML - Business Use Cases** | [`examples/analytical/tabular`](./examples/analytical/tabular) | Customer analytics (gbt_telco_churn), Marketing (gbt_upsell_propensity), Risk scoring (gbt_credit_default), Regression (gbt_housing_prices), Recommendation (ranking_lightgbm), Survival (pycox_retention), Anomaly detection (pyod_fraud_detection), Feature engineering (autofe_structured) | ‚úÖ |\n\n#### ü§ñ Agentic (AI Agents & Workflows) - Experimental\n\n| Capability | Example Directory | Implementations | Status |\n|------------|-------------------|-----------------|--------|\n| **RAG & Knowledge** | [`examples/agentic/rag`](./examples/agentic/rag) | langgraph_helpdesk | üß™ |\n| **LlamaIndex Agents** | [`examples/agentic/llamaindex`](./examples/agentic/llamaindex) | graph_rag_cognee, jsonalyze_structured_qa, react_finance_query_agent | üß™ |\n| **Prompt Optimization** | [`examples/agentic/optimization`](./examples/agentic/optimization) | dspy_gepa_promptops, mcts_prompt_agent | üß™ |\n| **Tool Use & Coding** | [`examples/agentic`](./examples/agentic) | Code (autogen_pair_programmer), Data analysis (smolagents_data_analyst), Local AI (llamacpp_assistant) | üß™ |\n\nNote: These examples are regularly updated and tested to ensure compatibility with the latest dependencies. Each example may contain one or more implementations using different frameworks or models. To start with, we recommend starting with the Image Classification example.\n\n## Browse Examples\n\n**Via GitHub:** Browse [perception/](./examples/perception/), [generative/](./examples/generative/), [analytical/](./examples/analytical/), or [agentic/](./examples/agentic/) directories\n\n**Via CLI:** Install with `pip install .` then search by keyword or tag:\n\n```bash\ncvl list -k gpt           # search by keyword\ncvl list --tag pytorch    # search by tag\ncvl list -k resnet\n```\n\n## Running an Example\n\nYou can run examples using the `cvl` CLI or directly with bash scripts:\n\n**Option 1: Using cvl CLI (recommended)**\n\n```bash\ncvl run image-classification-torch build\ncvl run image-classification-torch train\ncvl run image-classification-torch predict\n```\n\n**Option 2: Using bash scripts directly**\n\n```bash\ncd examples/perception/image_classification/torch\nbash build.sh\nbash train.sh\nbash predict.sh\n```\n\n**Examples:**\n\n```bash\n# Train a small LLM on Shakespeare text (Andrej Karpathy's nanoGPT)\ncvl run nanogpt train --max_iters=1000 --batch_size=16\n\n# Fine-tune Salesforce Moirai foundation model on time series data\ncvl run analytical/time_series/uni2ts_finetune train\n\n# Run zero-shot forecasting with Moirai\ncvl run analytical/time_series/moirai_zero_shot forecast\n\n# Document AI inference (IBM Granite-Docling)\ncvl run granite-docling predict -i input_pdf=@document.pdf\n```\n\nFor detailed instructions and available options, see the README.md in each example directory.\n\n**License Note:** Each example may reference projects with different licenses. Check the license file in each example directory.\n\n## Benefits of Using CVlization\n\n**Centralized Caching:** All examples use `~/.cache/cvlization/` for models and datasets, avoiding re-downloads across examples:\n- Automatic caching for HuggingFace Hub, PyTorch, and custom downloads\n- Managed by build scripts - no manual setup required\n- Shared across all examples to save disk space and bandwidth\n\n**Self-Contained Docker Environments:** Each example is isolated with pinned dependencies:\n- We battled CUDA versions and dependency conflicts so you don't have to\n- Source code is mounted at runtime (not baked into images) for easy iteration\n- Dependency versions are pinned where possible for reproducibility\n\n**Production-Ready Patterns:** Copy what works into your projects:\n- Consistent build/train/predict script structure across all examples\n- Battle-tested configurations for 90+ AI capabilities\n- Examples regularly updated and tested for compatibility\n\n## Requirements\n\n- Docker ([Install Docker](https://docs.docker.com/get-docker/))\n- NVIDIA GPU (most examples require 16GB+ VRAM; A10 or better recommended)\n- nvidia-docker for GPU access\n  ```bash\n  # Ubuntu\n  sudo apt-get install -y nvidia-container-toolkit\n  ```\n\n## Use CVlization on Colab\n\nNo installation needed - run examples directly in Google Colab:\n[Colab notebook: CIFAR-10 classification](https://colab.research.google.com/drive/1FkZcZnJC_z-PuFSYM91kU1-d63-LecMJ?usp=sharing)\n\n## For Contributors\n\nCVlization includes Claude Code skills for AI-assisted development and automated verification:\n\n- **`verify-training-pipeline`** - Validates training examples are properly structured, build successfully, train without errors, and log appropriate metrics\n- **`verify-inference-example`** - Validates inference examples build correctly and run inference successfully\n\nThese skills enable Claude to automatically verify examples end-to-end, helping maintain code quality across the repository.\n\n**Contributing Guidelines:**\n- See [CONTRIBUTING.md](./CONTRIBUTING.md) for standardization patterns and best practices\n- All examples should follow the build/train/predict script pattern\n- Use verification metadata in `example.yaml` to track testing status\n\n## Project Structure\n\n- `examples/`: Contains various computer vision and language processing examples\n- `bin/`: Shell scripts for building, running, and testing\n- `cvlization/`: The core library (legacy)\n- `data/`: Sample datasets\n- `doc/`: Project documentation\n- `tests/`: Unit and integration tests\n\n## CVlization Library\n\nThe `cvlization/` directory provides optional reusable components:\n\n**Available:**\n- Training pipeline abstractions (image classification, object detection, LLMs, diffusion)\n- Dataset builders with caching (PyTorch, TensorFlow, HuggingFace)\n- Model factories (Torchvision, MMDetection, MMSegmentation)\n- Utilities (metrics, logging, download helpers)\n\n**Installation:** `pip install -e .`\n\n**Note:** Examples are self-contained and don't require the library. For production use, copying example code directly is often simpler than depending on the library package.\n\n## Documentation\n\nDetailed documentation can be found in the `doc/` directory:\n\n- [Computer vision model training workflow and quality checks](./doc/Computer%20vision%20model%20training%20workflow%20and%20quality%20checks.pdf)\n- [Multi-task multi-input models: a common pattern](./doc/Multi-task%20multi-input%20models_%20a%20common%20pattern.pdf)\n- [Reusable model components](./doc/reusable_model_components.md)\n\n## Licenses\n\n**CVlization Library & CLI:** MIT License\n- The `cvlization` package and `cvl` CLI tool are released under the MIT License\n- Safe for commercial use\n\n**Examples Directory:** Mixed Licenses\n- Examples may reference projects with various licenses (copyleft, non-commercial, etc.)\n- Examples are NOT included when you `pip install cvlization`\n- Always check the license file in each example directory before using in production\n\n**Note:** Each example packages different open-source projects with their own licenses. Review licenses carefully for your use case.\n",
  "installCommand": "git clone https://github.com/kungfuai/CVlization ~/.claude/skills/cvlization",
  "defaultBranch": "main",
  "hasMarketplaceJson": false,
  "skillPath": "README.md"
}