{
  "id": "4cebceea586b416d",
  "slug": "multi-mcp",
  "name": "Multi_mcp",
  "description": "Multi-Model chat, code review and analysis MCP Server for Claude Code",
  "author": "religa",
  "authorAvatar": "https://avatars.githubusercontent.com/u/5545059?v=4",
  "repoUrl": "https://github.com/religa/multi_mcp",
  "repoFullName": "religa/multi_mcp",
  "stars": 5,
  "forks": 1,
  "category": "ai-ml",
  "categories": [
    "ai-ml"
  ],
  "tags": [
    "agents",
    "ai",
    "ai-agents",
    "ai-agents-cli",
    "ai-code-review",
    "anthropic",
    "claude",
    "claude-commands",
    "claude-mcp",
    "claude-skills",
    "code-analysis",
    "developer-tools",
    "gemini",
    "llm",
    "llm-orchestration",
    "mcp",
    "mcp-agent",
    "multi-agent",
    "multi-model",
    "openai"
  ],
  "tier": 3,
  "status": "active",
  "createdAt": "2025-11-29T16:45:57Z",
  "updatedAt": "2025-12-09T07:05:47Z",
  "lastCommitAt": "2025-12-09T07:05:43Z",
  "source": "github-topic",
  "collectedAt": "2025-12-11T09:41:43.843Z",
  "authorUrl": "https://github.com/religa",
  "license": "MIT",
  "readme": "# Multi-MCP: Multi-Model Code Review and Analysis MCP Server for Claude Code\n\n[![CI](https://github.com/religa/multi_mcp/workflows/CI/badge.svg)](https://github.com/religa/multi_mcp/actions)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)\n\nA **multi-model AI orchestration MCP server** for **automated code review** and **LLM-powered analysis**. Multi-MCP integrates with **Claude Code CLI** to orchestrate multiple AI models (OpenAI GPT, Anthropic Claude, Google Gemini) for **code quality checks**, **security analysis** (OWASP Top 10), and **multi-agent consensus**. Built on the **Model Context Protocol (MCP)**, this tool enables Python developers and DevOps teams to automate code reviews with AI-powered insights directly in their development workflow.\n\n![Demo Video](https://github.com/user-attachments/assets/39c3f100-e20d-4c3d-8130-b01c401f2d29)\n\n## Features\n\n- **üîç Code Review** - Systematic workflow with OWASP Top 10 security checks and performance analysis\n- **üí¨ Chat** - Interactive development assistance with repository context awareness\n- **üîÑ Compare** - Parallel multi-model analysis for architectural decisions\n- **üé≠ Debate** - Multi-agent consensus workflow (independent answers + critique)\n- **ü§ñ Multi-Model Support** - OpenAI GPT, Anthropic Claude, Google Gemini, and OpenRouter\n- **üñ•Ô∏è CLI & API Models** - Mix CLI-based (Gemini CLI, Codex CLI) and API models\n- **üè∑Ô∏è Model Aliases** - Use short names like `mini`, `sonnet`, `gemini`\n- **üßµ Threading** - Maintain context across multi-step reviews\n\n## How It Works\n\nMulti-MCP acts as an **MCP server** that Claude Code connects to, providing AI-powered code analysis tools:\n\n1. **Install** the MCP server and configure your AI model API keys\n2. **Integrate** with Claude Code CLI automatically via `make install`\n3. **Invoke** tools using natural language (e.g., \"multi codereview this file\")\n4. **Get Results** from multiple AI models orchestrated in parallel\n\n## Performance\n\n**Fast Multi-Model Analysis:**\n- ‚ö° **Parallel Execution** - 3 models in ~10s (vs ~30s sequential)\n- üîÑ **Async Architecture** - Non-blocking Python asyncio\n- üíæ **Conversation Threading** - Maintains context across multi-step reviews\n- üìä **Low Latency** - Response time = slowest model, not sum of all models\n\n## Quick Start\n\n**Prerequisites:**\n- Python 3.13+\n- [uv package manager](https://github.com/astral-sh/uv)\n- API key for at least one provider (OpenAI, Anthropic, Google, or OpenRouter)\n\n**Installation:**\n\n```bash\n# Clone and install\ngit clone https://github.com/religa/multi_mcp.git\ncd multi_mcp\nmake install\n\n# The installer will:\n# 1. Install dependencies (uv sync)\n# 2. Configure your .env file (from .env.example)\n# 3. Automatically add to Claude Code config (requires jq)\n# 4. Test the installation\n```\n\nAfter installation, restart Claude Code and type `/multi` to see available commands.\n\n## Configuration\n\n**Environment Configuration:**\n\nEdit `.env` with your API keys:\n\n```bash\n# API Keys (configure at least one)\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\nGEMINI_API_KEY=...\nOPENROUTER_API_KEY=sk-or-...\n\n# Model Configuration\nDEFAULT_MODEL=gpt-5-mini\nDEFAULT_MODEL_LIST=gpt-5-mini,gemini-2.5-flash\n```\n\n## Usage Examples\n\nOnce installed in Claude Code, you can use these commands:\n\n**üí¨ Chat** - Interactive development assistance:\n```\nCan you ask Multi chat what's the answer to life, universe and everything?\n```\n\n**üîç Code Review** - Analyze code with specific models:\n```\nCan you multi codereview this module for code quality and maintainability using gemini-3 and codex?\n```\n\n**üîÑ Compare** - Get multiple perspectives (uses default models):\n```\nCan you multi compare the best state management approach for this React app?\n```\n\n**üé≠ Debate** - Deep analysis with critique:\n```\nCan you multi debate the best project code name for this project?\n```\n\n## Enabling Allowlist\n\nEdit `~/.claude/settings.json` and add the following lines to `permissions.allow` to enable Claude Code to use Multi MCP without blocking for user permission:\n\n```json\n{\n  \"permissions\": {\n    \"allow\": [\n      ...\n      \"mcp__multi__chat\",\n      \"mcp__multi__codereview\",\n      \"mcp__multi__compare\",\n      \"mcp__multi__debate\",\n      \"mcp__multi__models\"\n    ],\n  },\n  \"env\": {\n    \"MCP_TIMEOUT\": \"300000\",\n    \"MCP_TOOL_TIMEOUT\": \"300000\"\n  },\n}\n```\n\n## Model Aliases\n\nUse short aliases instead of full model names:\n\n| Alias | Model | Provider |\n|-------|-------|----------|\n| `mini` | gpt-5-mini | OpenAI |\n| `sonnet` | claude-sonnet-4.5 | Anthropic |\n| `gpt` | gpt-5.1 | OpenAI |\n| `gemini` | gemini-3-pro-preview | Google |\n| `flash` | gemini-2.5-flash | Google |\n\n## CLI Models\n\nMulti-MCP can execute **CLI-based AI models** (like Gemini CLI or Codex CLI) alongside API models. CLI models run as subprocesses and work seamlessly with all existing tools.\n\n**Benefits:**\n- Use models with full tool access (file operations, shell commands)\n- Mix API and CLI models in `compare` and `debate` workflows\n- Leverage local CLIs without API overhead\n\n**Configuration:**\n\nCLI models are defined in `config/models.yaml` with `provider: cli`:\n\n```yaml\ngemini-cli:\n  provider: cli\n  cli_command: gemini\n  cli_args:\n    - \"-o\"\n    - \"json\"\n    - \"--yolo\"\n  cli_env:\n    GEMINI_API_KEY: \"${GEMINI_API_KEY}\"\n  cli_parser: json\n  aliases:\n    - gem-cli\n  notes: \"Gemini CLI with full tool access\"\n\ncodex-cli:\n  provider: cli\n  cli_command: codex\n  cli_args:\n    - \"exec\"\n    - \"--json\"\n    - \"--dangerously-bypass-approvals-and-sandbox\"\n  cli_env: {}\n  cli_parser: jsonl\n  aliases:\n    - cx-cli\n  notes: \"Codex CLI with full tool access\"\n```\n\n**Prerequisites:**\n\nCLI models require the respective CLI tools to be installed:\n\n```bash\n# Install Gemini CLI\npip install google-generativeai-cli\n\n# Install Codex CLI (Claude Code CLI)\nnpm install -g @anthropics/codex-cli\n```\n\n## CLI Usage (Experimental)\n\nMulti-MCP includes a standalone CLI for code review without needing an MCP client.\n\n‚ö†Ô∏è **Note:** The CLI is experimental and under active development.\n\n```bash\n# Review a directory\nmulti src/\n\n# Review specific files\nmulti src/server.py src/config.py\n\n# Use a different model\nmulti --model mini src/\n\n# JSON output for CI/pipelines\nmulti --json src/ > results.json\n\n# Verbose logging\nmulti -v src/\n\n# Specify project root (for CLAUDE.md loading)\nmulti --base-path /path/to/project src/\n```\n\n## Why Multi-MCP?\n\n| Feature | Multi-MCP | Single-Model Tools |\n|---------|-----------|-------------------|\n| Parallel model execution | ‚úÖ | ‚ùå |\n| Multi-model consensus | ‚úÖ | Varies |\n| Model debates | ‚úÖ | ‚ùå |\n| CLI + API model support | ‚úÖ | ‚ùå |\n| OWASP security analysis | ‚úÖ | Varies |\n\n\n## Troubleshooting\n\n**\"No API key found\"**\n- Add at least one API key to your `.env` file\n- Verify it's loaded: `uv run python -c \"from src.config import settings; print(settings.openai_api_key)\"`\n\n**Integration tests fail**\n- Set `RUN_E2E=1` environment variable\n- Verify API keys are valid and have sufficient credits\n\n**Debug mode:**\n```bash\nexport LOG_LEVEL=DEBUG # INFO is default\nuv run python src/server.py\n```\n\nCheck logs in `logs/server.log` for detailed information.\n\n## FAQ\n\n**Q: Do I need all three AI providers?**\nA: No, just one API key (OpenAI, Anthropic, or Google) is enough to get started.\n\n**Q: Does it truly run in parallel?**\nA: Yes! When you use `codereview`, `compare` or `debate` tools, all models are executed concurrently using Python's `asyncio.gather()`. This means you get responses from multiple models in the time it takes for the slowest model to respond, not the sum of all response times.\n\n**Q: How many models can I run at the same time?**\nA: There's no hard limit! You can run as many models as you want in parallel. In practice, 2-5 models work well for most use cases. All tools use your configured default models (typically 2-3), but you can specify any number of models you want.\n\n## Contributing\n\nWe welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for:\n- Development setup\n- Code standards\n- Testing guidelines\n- Pull request process\n\n**Quick start:**\n```bash\ngit clone https://github.com/YOUR_USERNAME/multi_mcp.git\ncd multi_mcp\nuv sync --extra dev\nmake check && make test\n```\n\n## License\n\nMIT License - see LICENSE file for details\n\n## Links\n\n- [Issue Tracker](https://github.com/religa/multi_mcp/issues)\n- [Contributing Guide](CONTRIBUTING.md)\n",
  "installCommand": "git clone https://github.com/religa/multi_mcp ~/.claude/skills/multi-mcp",
  "defaultBranch": "main",
  "hasMarketplaceJson": false,
  "skillPath": "README.md"
}