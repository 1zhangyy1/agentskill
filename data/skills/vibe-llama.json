{
  "id": "4aeb2d76953c0acd",
  "slug": "vibe-llama",
  "name": "Vibe Llama",
  "description": "Vibe-coding tools for the LlamaIndex ecosystem",
  "author": "run-llama",
  "authorAvatar": "https://avatars.githubusercontent.com/u/130722866?v=4",
  "repoUrl": "https://github.com/run-llama/vibe-llama",
  "repoFullName": "run-llama/vibe-llama",
  "stars": 172,
  "forks": 27,
  "category": "ai-ml",
  "categories": [
    "ai-ml"
  ],
  "tags": [],
  "tier": 2,
  "status": "maintained",
  "createdAt": "2025-08-19T12:17:16Z",
  "updatedAt": "2025-12-09T21:57:04Z",
  "lastCommitAt": "2025-11-03T16:27:23Z",
  "source": "github-search",
  "collectedAt": "2025-12-18T03:35:32.960Z",
  "authorUrl": "https://github.com/run-llama",
  "license": "MIT",
  "readme": "# vibe-llama\n\n> [!NOTE]\n>\n> _Find the documentation about vibe-llama-core (core utilities for downloading documentation and templates) [here](./packages/vibe-llama-core/README.md)_\n\n**vibe-llama** is a set of tools that are designed to help developers build working and reliable applications with [LlamaIndex](https://github.com/run-llama/llama_index), [LlamaCloud Services](https://github.com/run-llama/llama_cloud_services) and [llama-index-workflows](https://github.com/run-llama/workflows-py).\n\nThis command-line tool provides two main capabilities:\n\n**Context Injection:** Add relevant LlamaIndex context as rules to any coding agent of your choice (think Cursor, Claude Code, GitHub Copilot etc.). You select a coding agent and the LlamaIndex services you're working with, and vibe-llama generates rule files that give your AI assistant up-to-date knowledge about APIs, best practices, and common patterns.\n\nOnce you've made your choice, `vibe-llama` will generate a rule file for your coding agent. For example, if you selected Cursor, a new rule will be added to `.cursor/rules`. Now, all of the context and instructions about your chosen LlamaIndex service will be available to your coding agent of choice.\n\n**Workflow Generation:** An interactive CLI agent that helps you build document-processing workflows from scratch. Describe what you want in natural language, provide reference documents, and get complete workflow code with detailed explanations.\n\n## Installation\n\n**User settings**\n\nYou can install and run **vibe-llama** using `uv`:\n\n```bash\nuvx vibe-llama@latest --help\n```\n\nOr you can use `pip` to install it first and run it in a second moment:\n\n```bash\npip install vibe-llama\n```\n\n**Developer settings**\n\nClone the GitHub repository:\n\n```bash\ngit clone https://github.com/run-llama/vibe-llama\ncd vibe-llama\n```\n\nBuild and install the project:\n\n```bash\nuv build\n```\n\nFor regular installation:\n\n```bash\nuv pip install dist/*.whl\n```\n\nFor editable installation (development):\n\n```bash\n# Activate virtual environment first\nuv venv\nsource .venv/bin/activate  # On Unix/macOS\n\n# Then install in editable mode\nuv pip install -e .\n```\n\n## Usage\n\n**vibe-llama** is a CLI command, and has the following subcommands:\n\n### starter\n\n`starter` provides your coding agents with up-to-date documentation about LlamIndex, LlamaCloud Services and llama-index-workflows, so that they can build reliable and working applications! You can launch a terminal user interface by running `vibe-llama starter` and select your desired coding agents and services from there, or you can directly pass your agent (-a, --agent flag) and chosen service (-s, --service flag) from command line interface.\n\nUse the `-v`/`--verbose` flag (independently from TUI or CLI) if you want verbose logging of what processes are being executed while the application runs.\n\nUse the `-w`/`--overwrite` flag (works only from CLI) if you want to overwrite local files with the incoming ones downloaded by `vibe-llama starter`. On the TUI, you will be prompted to choose whether you want to overwrite existing files or not.\n\nIf you choose Claude Code as your coding agent, you can also use the `--skill` flag (also repeated multiple times) to download one or more of the [available skills](./documentation/skills/):\n\n- PDF processing with LlamaParse\n- Structured data extraction with LlamaExtract\n- Documents classification with LlamaClassify\n- Information retrieval with LlamaCloud Index\n- Bootstrapping, serving, deploying and managing workflows with llamactl\n\nOn the TUI, you will be asked for what skills you want to download.\n\n> [!IMPORTANT]\n>\n> Skills are meant to be customized, expanded and adapted to your own use case: you can build upon them and extend them to expand Claude Code's capabilities, even beyond our pre-set of rules.\n>\n> _Find out more about skills [on Claude Code documentation](https://docs.claude.com/en/docs/claude-code/skills)_\n\nIf you provide the `--allow_mcp_config` option, you will also download a generic MCP configuration (supported natively by Claude Code and Cursor), that will give you access to the local MCP provided by vibe-llama and to the general MCP provided by LlamaIndex over the entire documentation. This configuration can be easily adapted to other coding agents and can be saved to a specific path by providing the `--mcp_config_path` option (the default is `.mcp.json`).\n\nWith `starter`, you can also launch a local MCP server (at http://127.0.0.1:8000/mcp) using the `-m`/`--mcp` flag. This server exposes a tool (`get_relevant_context`) that allows you to retrieve relevant documentation content based on a specific query. If you are interested in interacting with vibe-llama MCP programmatically, you can check the [SDK guide](#vibellamamcpclient).\n\n**Example usage**\n\n```bash\nvibe-llama starter # Launch a TUI\nvibe-llama starter -a 'GitHub Copilot' -s LlamaIndex -v # Select GitHub Copilot and LlamaIndex and enable verbose logging\nvibe-llama starter -a 'Claude Code' -s llama-index-workflows -w # Select Claude Code and llama-index-workflows and allow to overwrite the existing CLAUDE.md\nvibe-llama starter -a 'Claude Code' -s llama-index-workflows --skill \"PDF Parsing\" --skill \"Llamactl Usage\" -w # download skills\nvibe-llama stater  -a 'Claude Code' -s llama-index-workflows --allow_mcp_config --mcp_config_path .claude/mcp.json # download the MCP configuration\nvibe-llama starter --mcp # Launch an MCP server\n```\n\n### docuflows\n\n`docuflows` is a CLI agent that enables you to build and edit workflows that are oriented to intelligent document processing (combining llama-index-workflows and LlamaCloud).\n\nIn order to use this command, you need to first set your OpenAI API key and your [LlamaCloud API key](https://cloud.llamaindex.ai) as environment variables. Optionally, if you wish to use Anthropic LLMs, you should also set the Anthropic API key in your environment.\n\n**On MacOS/Linux**\n\n```bash\nexport OPENAI_API_KEY=\"your-openai-api-key\"\nexport LLAMA_CLOUD_API_KEY=\"your-llama-cloud-api-key\"\n# optionally, for Anthropic usage\nexport ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\n```\n\n**On Windows**\n\n```powershell\nSet-Location Env:\n$Env:OPENAI_API_KEY=\"your-openai-api-key\"\n$Env:LLAMA_CLOUD_API_KEY=\"your-llama-cloud-api-key\"\n# optionally, for Anthropic usage\n$Env:ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\n```\n\nOnce you have the needed API keys in the environment, running `vibe-llama docuflows` will start a terminal interface where you will be able to interactively talk to the agent and create or edit document-centered workflows with the help of it.\n\n**Configuration**\n\nDuring an open session with `docuflows`, first of all you will be prompted to configure your LlamaCloud settings (project and organization ID are required for this step).\n\nYou can optionally configure other settings, by using slash commands directly from the terminal interface:\n\n- `/configure` will allow you to re-configure the LlamaCloud project settings, as well as the default output directory and the default references file path.\n- `/model` will allow you to choose the LLM to use. By default, `vibe-llama docuflows` uses GPT-4.1, but you can change it to other models from the GPT family or to Claude models.\n\nOnce you configured everything to your liking, you can start using the agent to create or edit workflows.\n\n**Usage**\n\n```bash\nvibe-llama docuflows\n```\n\n> [!NOTE]\n>\n> _`vibe-llama docuflows` uses **AGENTS.md** as an instructions file (located under `.vibe-llama/rules/`). If you wish, you can directly create AGENTS.md with the `starter` command, by selecting `vibe-llama docuflows` as your agent. Alternatively, if an AGENTS.md is not present in your environment, `vibe-llama docuflows` will create one on the fly._\n\nDuring the editing or generation process, you will be asked to provide reference files for your workflow (e.g. an invoice file if you are asking for an invoice-processing workflow), so make sure to prepare them.\n\nOnce the workflow generation/editing is finished, you will be able to save the code and the code-related explanation in a folder that will be created under `generated_workflows/`. In the folder you will find a `workflow.py` file, containing the code, and a `runbook.md` file, containing instructions and explanations related to the code.\n\n### scaffold\n\n`scaffold` is a command that allows you to generate working examples of AI-powered workflows for a variety of use cases.\n\nYou can use it from command line, and you can pass the `-u`/`--use_case` flag to select the use case and `-p`/`--path` flag to define the path where the example workflow will be stored (defaults to `.vibe-llama/scaffold`).\n\nAlternatively, you can launch a terminal user interface by running `vibe-llama scaffold`.\n\nOnce you chose the use case to download and the path to save the code to, `scaffold` will populate the specified path with a `workflow.py` (containing the actual workflow code), a `README.md` (with explanation on how to set up and run the workflow, as well as on the workflow structure) and a `pyproject.toml` with all the project details.\n\n**Example usage**\n\n```bash\nvibe-llama scaffold --use_case document_parsing --path examples/document_parsing_workflow/ # save the document parsing use case to examples/document_parsing_workflow/\nvibe-llama scaffold # launch the terminal interface\n```\n\n> [!NOTE]\n>\n> Starters are pulled from GitHub template repositories via [copier](https://copier.readthedocs.io/en/stable/):\\_\n>\n> - `https://github.com/run-llama/template-workflow-basic`\n> - `https://github.com/run-llama/template-workflow-document-parsing`\n> - `https://github.com/run-llama/template-workflow-human-in-the-loop`\n> - `https://github.com/run-llama/template-workflow-invoice-extraction`\n> - `https://github.com/run-llama/template-workflow-rag`\n> - `https://github.com/run-llama/template-workflow-web-scraping`\n\n## SDK\n\nvibe-llama also comes with a programmatic interface that you can call within your python scripts.\n\n### `VibeLlamaStarter`\n\nTo replicate the `starter` command on the CLI and fetch all the needed instructions for your coding agents, you can use the following code:\n\n```python\nfrom vibe_llama.sdk import VibeLlamaStarter\n\nstarter = VibeLlamaStarter(\n    agents=[\"GitHub Copilot\", \"Cursor\"],\n    services=[\"LlamaIndex\", \"llama-index-workflows\"],\n)\n\nawait starter.write_instructions(\n    verbose=True, max_retries=20, retry_interval=0.7\n)\n```\n\n### `VibeLlamaMCPClient`\n\n> [!NOTE]\n>\n> _To interact with vibe-llama MCP server you can use any MCP client of your liking_.\n\nThis class implements an MCP client to interact directly and in a well-integrated way with vibe-llama MCP server.\n\nYou can use it as follows:\n\n```python\nfrom vibe_llama.sdk import VibeLlamaMCPClient\n\nclient = VibeLlamaMCPClient()\n\n# list the available tools\nawait client.list_tools()\n\n# retrieve specific documentation content\nawait client.retrieve_docs(query=\"Parsing pre-sets in LlamaParse\")\n\n# retrieve a certain number of matches\nawait client.retrieve_docs(query=\"Human in the loop\", top_k=4)\n\n# retrieve matches and parse the returned XML string\nresult = await client.retrieve_docs(\n    query=\"Workflow Design Patterns\", top_k=3, parse_xml=True\n)\nif \"result\" in result:\n    print(result[\"result\"])  # -> List of the top three matches for your query\nelse:\n    print(result[\"error\"])  # -> List of error messages\n```\n\n### `VibeLlamaDocsRetriever`\n\nThis class implements a retriever for vibe-llama documentation, leveraging BM25 (enhanced with stemming) for lightweight, on-disk indexing and retrieval.\n\nYou can use it as follows:\n\n```python\nfrom vibe_llama.sdk import VibeLlamaDocsRetriever\n\nretriever = VibeLlamaDocsRetriever()\n\n# retrieve a maximum of 10 relevant documents pertaining to the query 'What is LlamaExtract?'\nawait retriever.retrieve(query=\"What is LlamaExtract?\", top_k=10)\n```\n\n### `VibeLlamaScaffold`\n\nVibeLlamaScaffold allows you to download human-curated, end-to-end workflows templates for various use cases.\n\nYou can use it as follows:\n\n```python\nfrom vibe_llama.sdk import VibeLlamaScaffold\n\nscaffolder = VibeLlamaScaffold(\n    colored_output=True\n)  # you can enable/disable colored output\n\nawait scaffolder.get_template(\n    template_name=\"invoice_extraction\",\n    save_path=\"examples/invoice_extraction/\",\n)  # if you do not provide a `save_path`, it will default to `.vibe-llama/scaffold`\n```\n\n## Contributing\n\nWe welcome contributions! Please read our [Contributing Guide](CONTRIBUTING.md) to get started.\n\n## License\n\nThis project is licensed under the [MIT License](./LICENSE).\n",
  "installCommand": "git clone https://github.com/run-llama/vibe-llama ~/.claude/skills/vibe-llama",
  "defaultBranch": "main",
  "hasMarketplaceJson": false,
  "skillPath": "README.md"
}