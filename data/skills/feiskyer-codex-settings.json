{
  "id": "e42543aa52f5cde2",
  "slug": "feiskyer-codex-settings",
  "name": "Codex Settings",
  "description": "OpenAI Codex CLI settings, configurations, skills and prompts for vibe coding",
  "author": "feiskyer",
  "authorAvatar": "https://avatars.githubusercontent.com/u/676637?v=4",
  "repoUrl": "https://github.com/feiskyer/codex-settings",
  "repoFullName": "feiskyer/codex-settings",
  "stars": 69,
  "forks": 12,
  "category": "ai-ml",
  "categories": [
    "ai-ml"
  ],
  "tags": [
    "agentic-ai",
    "agents",
    "ai",
    "claude-code",
    "claude-skills",
    "codex",
    "copilot",
    "litellm",
    "openai",
    "spec-driven-development",
    "vibe-coding"
  ],
  "tier": 2,
  "status": "active",
  "createdAt": "2025-10-06T08:40:43Z",
  "updatedAt": "2025-12-17T19:12:13Z",
  "lastCommitAt": "2025-12-17T14:30:57Z",
  "source": "github-topic",
  "collectedAt": "2025-12-18T03:25:49.408Z",
  "authorUrl": "https://github.com/feiskyer",
  "license": "MIT",
  "readme": "# OpenAI Codex CLI Settings and Custom Prompts\n\nA curated collection of configurations, skills and custom prompts for [OpenAI Codex CLI](https://github.com/openai/codex), designed to enhance your development workflow with various model providers and reusable prompt templates.\n\n> For Claude Code settings, skills, agents and custom commands, please refer [feiskyer/claude-code-settings](https://github.com/feiskyer/claude-code-settings).\n\n## Overview\n\nThis repository provides:\n\n- **Flexible Configuration**: Support for multiple model providers (LiteLLM/Copilot proxy, ChatGPT subscription, Azure OpenAI, OpenRouter, ModelScope, Kimi)\n- **Custom Prompts**: Reusable prompt templates for common development tasks\n- **Skills (Experimental)**: Discoverable instruction bundles for specialized tasks (image generation, YouTube transcription, spec-driven workflows)\n- **Best Practices**: Pre-configured settings optimized for development workflows\n- **Easy Setup**: Simple installation and configuration process\n\n## Quick Start\n\n### Installation\n\n```bash\n# Backup existing Codex configuration (if any)\nmv ~/.codex ~/.codex.bak\n\n# Clone this repository to ~/.codex\ngit clone https://github.com/feiskyer/codex-settings.git ~/.codex\n\n# Or symlink if you prefer to keep it elsewhere\nln -s /path/to/codex-settings ~/.codex\n```\n\n### Basic Configuration\n\nThe default `config.toml` uses LiteLLM as a gateway. To use it:\n\n1. Install LiteLLM and Codex CLI:\n\n   ```bash\n   pip install -U 'litellm[proxy]'\n   npm install -g @openai/codex\n   ```\n\n1. Create a LiteLLM config file (full example [litellm_config.yaml](litellm_config.yaml)):\n\n   ```yaml\n   general_settings:\n     master_key: sk-dummy\n   litellm_settings:\n     drop_params: true\n   model_list:\n   - model_name: gpt-5\n     litellm_params:\n       model: github_copilot/gpt-5\n       extra_headers:\n         editor-version: \"vscode/1.104.3\"\n         editor-plugin-version: \"copilot-chat/0.26.7\"\n         Copilot-Integration-Id: \"vscode-chat\"\n         user-agent: \"GitHubCopilotChat/0.26.7\"\n         x-github-api-version: \"2025-04-01\"\n   ```\n\n1. Start LiteLLM proxy:\n\n   ```bash\n   litellm --config ~/.codex/litellm_config.yaml\n   # Runs on http://localhost:4000 by default\n   ```\n\n1. Run Codex:\n\n   ```bash\n   codex\n   ```\n\n## Configuration Files\n\n### Main Configuration\n\n- [config.toml](config.toml): Default configuration using LiteLLM gateway\n  - Model: `gpt-5` via `model_provider = \"github\"` (Copilot proxy on `http://localhost:4000`)\n  - Approval policy: `on-request`; reasoning summary: `detailed`; reasoning effort: `high`; raw agent reasoning visible\n  - MCP servers: `claude` (local), `exa` (hosted), `chrome` (DevTools over `npx`)\n\n### Alternative Configurations\n\nLocated in `configs/` directory:\n\n- [OpenAI ChatGPT](configs/chatgpt.toml): Use ChatGPT subscription provider\n- [Azure OpenAI](configs/azure.toml): Use Azure OpenAI service provider\n- [Github Copilot](configs/github-copilot.toml): Use Github Copilot via LiteLLM proxy\n- [OpenRouter](configs/openrouter.toml): Use OpenRouter provider\n- [Model Scope](configs/modelscope.toml): Use ModelScope provider\n- [Kimi](configs/kimi.toml): Use Moonshot Kimi provider\n\nTo use an alternative config:\n\n```bash\n# Take ChatGPT for example\ncp ~/.codex/configs/chatgpt.toml ~/.codex/config.toml\ncodex\n```\n\n## Custom Prompts\n\nCustom prompts are stored in the `prompts/` directory. Access them via the `/prompts:` slash menu in Codex.\n\n- `/prompts:deep-reflector` - Analyze development sessions to extract learnings, patterns, and improvements for future interactions.\n- `/prompts:insight-documenter [breakthrough]` - Capture and document significant technical breakthroughs into reusable knowledge assets.\n- `/prompts:instruction-reflector` - Analyze and improve Codex instructions in AGENTS.md based on conversation history.\n- `/prompts:github-issue-fixer [issue-number]` - Systematically analyze, plan, and implement fixes for GitHub issues with PR creation.\n- `/prompts:github-pr-reviewer [pr-number]` - Perform thorough GitHub pull request code analysis and review.\n- `/prompts:ui-engineer [requirements]` - Create production-ready frontend solutions with modern UI/UX standards.\n- `/prompts:prompt-creator [requirements]` - Create Codex custom prompts with proper structure and best practices.\n\n### Creating Custom Prompts\n\n1. Create a new `.md` file in `~/.codex/prompts/`\n2. Use argument placeholders:\n   - `$1` to `$9`: Positional arguments\n   - `$ARGUMENTS`: All arguments joined by spaces\n   - `$$`: Literal dollar sign\n3. Restart Codex to load new prompts\n\n## Skills (Experimental)\n\nSkills are reusable instruction bundles that Codex automatically discovers at startup. Each skill has a name, description, and detailed instructions stored on disk. Codex injects only metadata (name, description, path) into context - the body stays on disk until needed.\n\n### How to Use Skills\n\nSkills are automatically loaded when Codex starts. To use a skill:\n\n1. **List all skills**: Use the `/skills` command to see all available skills\n\n   ```text\n   /skills\n   ```\n\n2. **Invoke a skill**: Use `$<skill-name> [prompt]` to invoke a skill with an optional prompt\n\n   ```text\n   $kiro-skill Create a feature spec for user authentication\n   $nanobanana-skill Generate an image of a sunset over mountains\n   ```\n\nSkills are stored in `~/.codex/skills/**/SKILL.md`. Only files named exactly `SKILL.md` are recognized.\n\n### Available Skills\n\n<details>\n<summary>claude-skill - Handoff task to Claude Code CLI</summary>\n\n#### [claude-skill](skills/claude-skill)\n\nNon-interactive automation mode for hands-off task execution using Claude Code. Use when you want to leverage Claude Code to implement features or review code.\n\n**Key Features:**\n\n- Multiple permission modes (default, acceptEdits, plan, bypassPermissions)\n- Autonomous execution without approval prompts\n- Streaming progress updates\n- Structured final summaries\n\n**Requirements:** Claude Code CLI installed (`npm install -g @anthropic-ai/claude-code`)\n\n</details>\n\n<details>\n<summary>autonomous-skill - Long-running task automation</summary>\n\n#### [autonomous-skill](skills/autonomous-skill)\n\nExecute complex, long-running tasks across multiple sessions using a dual-agent pattern (Initializer + Executor) with automatic session continuation.\n\n> Warning: workflows may pause when Codex requests permissions. Treat this as experimental; expect to babysit early runs and keep iterating on approvals/sandbox settings.\n\n**Key Features:**\n\n- Dual-agent pattern (Initializer creates task list, Executor completes tasks)\n- Auto-continuation across sessions with progress tracking\n- Task isolation with per-task directories (`.autonomous/<task-name>/`)\n- Progress persistence via `task_list.md` and `progress.md`\n- Non-interactive mode execution\n\n**Usage:**\n\n```bash\n# Start a new autonomous task\n~/.codex/skills/autonomous-skill/scripts/run-session.sh \"Build a REST API for todo app\"\n\n# Continue an existing task\n~/.codex/skills/autonomous-skill/scripts/run-session.sh --task-name build-rest-api-todo --continue\n\n# List all tasks\n~/.codex/skills/autonomous-skill/scripts/run-session.sh --list\n```\n\n</details>\n\n<details>\n<summary>nanobanana-skill - Image generation with Gemini</summary>\n\n#### [nanobanana-skill](skills/nanobanana-skill)\n\nGenerate or edit images using Google Gemini API via nanobanana. Use when creating, generating, or editing images.\n\n**Key Features:**\n\n- Image generation with various aspect ratios (square, portrait, landscape, ultra-wide)\n- Image editing capabilities\n- Multiple model options (gemini-3-pro-image-preview, gemini-2.5-flash-image)\n- Resolution options (1K, 2K, 4K)\n\n**Requirements:**\n\n- `GEMINI_API_KEY` configured in `~/.nanobanana.env`\n- Python3 with google-genai, Pillow, python-dotenv\n\n</details>\n\n<details>\n<summary>youtube-transcribe-skill - Extract YouTube subtitles</summary>\n\n#### [youtube-transcribe-skill](skills/youtube-transcribe-skill)\n\nExtract subtitles/transcripts from a YouTube video URL and save as a local file.\n\n**Key Features:**\n\n- Dual extraction methods: CLI (`yt-dlp`) and Browser Automation (fallback)\n- Automatic subtitle language selection (zh-Hans, zh-Hant, en)\n- Cookie handling for age-restricted content\n- Saves transcripts to local text files\n\n**Requirements:**\n\n- `yt-dlp` (for CLI method), or\n- Browser automation MCP server (for fallback method)\n\n</details>\n\n<details>\n<summary>kiro-skill - Interactive feature development</summary>\n\n#### [kiro-skill](skills/kiro-skill)\n\nInteractive feature development workflow from idea to implementation. Creates requirements (EARS format), design documents, and implementation task lists.\n\n**Triggered by:** \"kiro\" or references to `.kiro/specs/` directory\n\n**Workflow:**\n\n1. **Requirements** → Define what needs to be built (EARS format with user stories)\n2. **Design** → Determine how to build it (architecture, components, data models)\n3. **Tasks** → Create actionable implementation steps (test-driven, incremental)\n4. **Execute** → Implement tasks one at a time\n\n**Storage:** Creates files in `.kiro/specs/{feature-name}/` directory\n\n</details>\n\n<details>\n<summary>spec-kit-skill - Constitution-based development</summary>\n\n#### [spec-kit-skill](skills/spec-kit-skill)\n\nGitHub Spec-Kit integration for constitution-based spec-driven development.\n\n**Triggered by:** \"spec-kit\", \"speckit\", \"constitution\", \"specify\", or references to `.specify/` directory\n\n**Prerequisites:**\n\n```bash\n# Install spec-kit CLI\nuv tool install specify-cli --from git+https://github.com/github/spec-kit.git\n\n# Initialize project\nspecify init . --ai codex\n```\n\n**7-Phase Workflow:**\n\n1. **Constitution** → Establish governing principles\n2. **Specify** → Define functional requirements\n3. **Clarify** → Resolve ambiguities (max 5 questions)\n4. **Plan** → Create technical strategy\n5. **Tasks** → Generate dependency-ordered tasks\n6. **Analyze** → Validate consistency (read-only)\n7. **Implement** → Execute implementation\n\n</details>\n\n## Configuration Options\n\n### Approval Policies\n\n- `untrusted`: Prompt for untrusted commands (recommended)\n- `on-failure`: Only prompt when sandbox commands fail\n- `on-request`: Model decides when to ask\n- `never`: Auto-approve all commands (use with caution)\n\n### Sandbox Modes\n\n- `read-only`: Can read files, no writes or network\n- `workspace-write`: Can write to workspace, network configurable\n- `danger-full-access`: Full system access (use in containers only)\n\n### Reasoning Settings\n\nFor reasoning-capable models (o3, gpt-5):\n\n- **Effort**: `minimal`, `low`, `medium`, `high`\n- **Summary**: `auto`, `concise`, `detailed`, `none`\n\n### Shell Environment\n\nControl which environment variables are passed to subprocesses:\n\n```toml\n[shell_environment_policy]\ninherit = \"all\"  # all, core, none\nexclude = [\"AWS_*\", \"AZURE_*\"]  # Exclude patterns\nset = { CI = \"1\" }  # Force-set values\n```\n\n## Advanced Features\n\n### Profiles\n\nDefine multiple configuration profiles:\n\n```toml\n[profiles.openrouter]\nmodel = \"gpt-5\"\nmodel_reasoning_effort = \"high\"\napproval_policy = \"on-request\"\nsandbox_mode = \"workspace-write\"\nmodel_provider = \"openrouter\"\n\n[profiles.github]\nmodel = \"gpt-5\"\nmodel_reasoning_effort = \"high\"\napproval_policy = \"on-request\"\nsandbox_mode = \"workspace-write\"\nmodel_provider = \"github\"\n\n[model_providers.github]\nname     = \"OpenAI\"\nbase_url = \"http://localhost:4000\"\nhttp_headers = { \"Authorization\"= \"Bearer sk-dummy\"}\nwire_api = \"chat\"\n\n[model_providers.openrouter]\nname     = \"OpenRouter\"\nbase_url = \"https://openrouter.ai/api/v1\"\nhttp_headers = { \"Authorization\"= \"Bearer [YOUR-API-KEY]\"}\nwire_api = \"chat\"\n```\n\nUse with: `codex --profile openrouter`\n\n### MCP Servers\n\nExtend Codex with Model Context Protocol servers:\n\n```toml\n[mcp_servers.context7]\ncommand = \"npx\"\nargs = [\"-y\", \"@upstash/context7-mcp@latest\"]\n```\n\n## Project Documentation\n\nCodex automatically reads `AGENTS.md` files in your project to understand context. Please always create one in your project root with `/init` command on your first codex run.\n\n## References\n\n- [Codex CLI Official Docs](https://developers.openai.com/codex/cli/)\n- [Codex GitHub Repository](https://github.com/openai/codex)\n- [LiteLLM Documentation](https://docs.litellm.ai/)\n\n## Contributing\n\nContributions welcome! Feel free to:\n\n- Add new custom prompts\n- Share alternative configurations\n- Improve documentation\n- Report issues and suggest features\n\n## LICENSE\n\nThis project is released under MIT License - See [LICENSE](LICENSE) for details.\n",
  "installCommand": "git clone https://github.com/feiskyer/codex-settings ~/.claude/skills/feiskyer-codex-settings",
  "defaultBranch": "main",
  "hasMarketplaceJson": false,
  "skillPath": "README.md"
}