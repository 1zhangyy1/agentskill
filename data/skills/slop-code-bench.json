{
  "id": "288542e8d584726f",
  "slug": "slop-code-bench",
  "name": "Slop Code Bench",
  "description": "SlopCodeBench: Measuring Code Erosion Under Iterative Specification Refinement",
  "author": "SprocketLab",
  "authorAvatar": "https://avatars.githubusercontent.com/u/79233274?v=4",
  "repoUrl": "https://github.com/SprocketLab/slop-code-bench",
  "repoFullName": "SprocketLab/slop-code-bench",
  "stars": 6,
  "forks": 0,
  "category": "testing",
  "categories": [
    "testing"
  ],
  "tags": [],
  "tier": 3,
  "status": "active",
  "createdAt": "2025-12-17T18:45:40Z",
  "updatedAt": "2025-12-26T18:32:13Z",
  "lastCommitAt": "2025-12-26T18:32:09Z",
  "source": "github-search",
  "collectedAt": "2025-12-27T03:35:28.834Z",
  "authorUrl": "https://github.com/SprocketLab",
  "license": "MIT",
  "readme": "<div align=\"center\">\n<h1 > SlopCodeBench (SCBench)</h1>\n\n  [![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)\n  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)\n  [![GitHub stars](https://img.shields.io/github/stars/SprocketLab/slop-code-bench)](https://github.com/SprocketLab/slop-code-bench/stargazers)\n\n  [üåê Website](https://www.scbench.ai) | [üìù Blog Post](https://gabeorlanski.github.io/posts/slop-code-bench)\n</div>\n\n![](assets/overview.png)\n---\n\n**SlopCodeBench** evaluates coding agents under iterative specification refinement: the agent implements a spec, then extends its own code as the spec changes. This exposes behaviors that single-shot benchmarks cannot measure, including path dependence, non-convergence, and trade-offs between explicit handling and structural stability. We release SCBench as an open, community-driven evaluation primitive rather than a finalized benchmark.\n\n\nWe actively want more problems, follow [the creating a problem guide and create a pr!](/docs/contributing-problems/).\n\n> [!NOTE]\n> This is an initial release. We're actively developing and welcome feedback via [GitHub Issues](https://github.com/SprocketLab/slop-code-bench/issues).\n\n## Prerequisites\n\nBefore installing, ensure you have:\n- **Python 3.12+** installed\n- **Docker** installed and running ([Get Docker](https://docs.docker.com/get-docker/))\n- An **API key** for your chosen agent (e.g., Anthropic, OpenAI, Google)\n- **8GB+ RAM** recommended for running evaluations\n- **10GB+ disk space** for Docker images and workspaces\n\n## üöÄ Install\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\ngit clone https://github.com/SprocketLab/slop-code-bench.git && cd slop-code-bench && uv sync\nexport ANTHROPIC_API_KEY=\"your-key\"\n\n# Run!\nuv run slop-code run \\\n  --agent claude_code \\\n  --model anthropic/opus-4.5 \\\n  --environment configs/environments/docker-python3.12-uv.yaml \\\n  --prompt configs/prompts/just-solve.jinja \\\n  --problem file_backup \\\n  --problem execution_server \\\n  thinking=low \\\n  version=2.0.51\n```\n\n**Parameter Reference:**\n- `thinking=none|low|medium|high` - Controls extended thinking budget based on agent.\n- `version=X.Y.Z` - Agent version to use.\n\nResults are saved to:\n```\noutputs/opus-4.5/claude_code-just-solve_low_{timestamp}/\n```\n\n**First Run:** Docker images build automatically for that _VERSION_ of the agent (5-10 minutes). Subsequent runs are faster.\n\n### Troubleshooting\n\n**Docker not found:**\n```bash\n# Check Docker is running\ndocker ps\n# If not running, start Docker Desktop or daemon\n```\n\n**API key not found:**\n```bash\n# Verify your environment variable is set\necho $ANTHROPIC_API_KEY\n# Or pass it directly\nANTHROPIC_API_KEY=\"your-key\" uv run slop-code run ...\n```\n\n**Out of disk space:**\n```bash\n# Clean up old Docker images\ndocker system prune -a\n```\n\nFor more issues, see [GitHub Issues](https://github.com/SprocketLab/slop-code-bench/issues).\n\n## üìä Evaluation\n\n**Evaluate a run:**\n```bash\nslop-code eval outputs/your-run-directory/\n```\n\n**Grade code quality with LLM judge:**\n```bash\nslop-code metrics judge \\\n  --rubric configs/rubrics/llm_judge.jsonl \\\n  --model <model on openrouter> \\\n  --criteria-template configs/rubrics/templates/criteria_with_pn.j2 \\\n  --prefix-template configs/rubrics/templates/no_expl.j2\n```\n\n## Contributing\n\nWe welcome contributions. Two ways to help:\n\n- **Add problems** ‚Äî Expand the benchmark with new evaluation scenarios. See the [Problem Tutorial](docs/problems/tutorial.md) and [Contributing Guide](CONTRIBUTING.md).\n- **Add agents** ‚Äî Integrate new coding agents. See the [Agent Guide](docs/agents/README.md) and [Contributing Guide](CONTRIBUTING.md).\n\nThis is early-stage software. Your contributions will shape its direction.\n\n## Documentation\n\n| Guide | Description |\n|-------|-------------|\n| [‚ùì FAQ](docs/FAQ.md) | Frequently asked questions |\n| [üìñ Problem Tutorial](docs/problems/tutorial.md) | Create your first problem (30 min hands-on) |\n| [üìã Quick Reference](docs/problems/quick-reference.md) | One-page cheat sheet for problem authoring |\n| [ü§ñ Agent Guide](docs/agents/README.md) | Configure agents, models, and credentials |\n| [üèóÔ∏è Architecture](docs/execution/README.md) | How sessions, workspaces, and runtimes work |\n| [‚úÖ Evaluation System](docs/evaluation/README.md) | Test cases, adapters, loaders, and verifiers |\n| [üí° Problem Design](docs/contributing-problems/README.md) | What makes a good evaluation problem |\n| [‚ö†Ô∏è Known Issues](docs/KNOWN_ISSUES.md) | Current limitations and workarounds |\n\n## Citing Us\n\nIf you found this useful, please cite us as:\n```bibtex\n@misc{slopcodebench,\n  title        = {SlopCodeBench: Measuring Code Erosion Under Iterative Specification Refinement},\n  author       = {Gabriel Orlanski and Devjeet Roy and Alexander Yun and \n                  Changho Shin and Alex Gu and Albert Ge and \n                  Dyah Adila and Aws Albarghouthi and Frederic Sala},\n  year         = {2025},\n  howpublished = {\\url{https://github.com/SprocketLab/slop-code-bench}},\n}\n```\n",
  "installCommand": "git clone https://github.com/SprocketLab/slop-code-bench ~/.claude/skills/slop-code-bench",
  "defaultBranch": "main",
  "hasMarketplaceJson": false,
  "skillPath": "README.md"
}